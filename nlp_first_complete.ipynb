{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzCg-JNZXu7i",
        "outputId": "1d3c2cd7-b548-4995-a9fd-0627ee21919a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "BT1sCttfYzVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"My name is Tanmay Ekade. I am in 3rd year at IIT Kharagpur. I am from Nashik, MH. It is India's wine capital!\"\n",
        "tokens = nltk.word_tokenize(sent)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYyWEJ-BX8zc",
        "outputId": "b5b999ee-6389-4ae7-85f7-dafca28f215d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Tanmay',\n",
              " 'Ekade',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'in',\n",
              " '3rd',\n",
              " 'year',\n",
              " 'at',\n",
              " 'IIT',\n",
              " 'Kharagpur',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'from',\n",
              " 'Nashik',\n",
              " ',',\n",
              " 'MH',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'India',\n",
              " \"'s\",\n",
              " 'wine',\n",
              " 'capital',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvtRvHdFqusp",
        "outputId": "a5ebde31-ac8d-4ce2-ab7b-b52139bcac97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import wordpunct_tokenize\n",
        "wordpunct_tokenize(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FOYiLuxsCja",
        "outputId": "2bcfece5-e7e2-4b07-a20d-76f7ed643d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Tanmay',\n",
              " 'Ekade',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'in',\n",
              " '3rd',\n",
              " 'year',\n",
              " 'at',\n",
              " 'IIT',\n",
              " 'Kharagpur',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'from',\n",
              " 'Nashik',\n",
              " ',',\n",
              " 'MH',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'India',\n",
              " \"'\",\n",
              " 's',\n",
              " 'wine',\n",
              " 'capital',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference as you can see is here -->\n",
        "\n",
        "India + 's (work_tok) ___ vs ___\n",
        "\n",
        "India + ' + s (workpunct_token)"
      ],
      "metadata": {
        "id": "tYlkXJDus_4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "sent_tokenize(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzOqJF64YmHr",
        "outputId": "d9727284-cfe9-4d40-eb26-7a5be4f647d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My name is Tanmay Ekade.',\n",
              " 'I am in 3rd year at IIT Kharagpur.',\n",
              " 'I am from Nashik, MH.',\n",
              " \"It is India's wine capital!\"]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming is the process of getting the root word for a particular word. Helpful in classification or sentiment analysis etc types of problems\n",
        "# for ex. working, worked, all basically have the same root word as \"work\".\n",
        "\n",
        "wordlist = [\"eating\", \"eaten\", \"eats\", \"writing\", \"write\", \"program\", \"programming\", \"history\", \"finally\", \"finalized\", \"do\", \"don't\", \"fairly\", \"sportingly\"]\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stem_port = PorterStemmer()\n",
        "\n",
        "for word in wordlist:\n",
        "  print(word + \"  ----->  \" + stem_port.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmKzHNNiZLR-",
        "outputId": "76f868e3-ca4a-44e5-8a8f-7000bb398601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating  ----->  eat\n",
            "eaten  ----->  eaten\n",
            "eats  ----->  eat\n",
            "writing  ----->  write\n",
            "write  ----->  write\n",
            "program  ----->  program\n",
            "programming  ----->  program\n",
            "history  ----->  histori\n",
            "finally  ----->  final\n",
            "finalized  ----->  final\n",
            "do  ----->  do\n",
            "don't  ----->  don't\n",
            "fairly  ----->  fairli\n",
            "sportingly  ----->  sportingli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#doesnt work for some words like history. problem is solved in lemmatization\n",
        "\n",
        "from nltk.stem import RegexpStemmer\n",
        "# removes prefixes or suffxies that we provide to reg_exp\n",
        "reg_stem = RegexpStemmer('ing$|s$|e$|able$', min=4) # list we provide\n",
        "\n",
        "reg_stem.stem(\"eating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hpdFfcbqwFDh",
        "outputId": "53afa025-9b62-4594-ccb1-393dc74430dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stem.stem(\"ingeating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ve7XpnmUxtpi",
        "outputId": "49fb0e85-e153-4472-c1a5-5c9d3add5ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stem = RegexpStemmer('ing|s$|e$|able$', min=4) # list we provide # $ basically means at last\n",
        "reg_stem.stem(\"ingeating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RDTLZPm3x5aW",
        "outputId": "88aa23c7-96ab-4897-ffe6-03d838523dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Snowball Stemmer gives better results compared to Porter Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "snow_stem = SnowballStemmer('english')\n",
        "for word in wordlist:\n",
        "  print(word + \" ----> \" + snow_stem.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c27zlTkVx-a5",
        "outputId": "9cb1c2bd-568c-412c-c50a-a1b49f7226ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ----> eat\n",
            "eaten ----> eaten\n",
            "eats ----> eat\n",
            "writing ----> write\n",
            "write ----> write\n",
            "program ----> program\n",
            "programming ----> program\n",
            "history ----> histori\n",
            "finally ----> final\n",
            "finalized ----> final\n",
            "do ----> do\n",
            "don't ----> don't\n",
            "fairly ----> fair\n",
            "sportingly ----> sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, last two words in snowball is correct but wrong in Porter one. But for both history is wrong. So relatively better"
      ],
      "metadata": {
        "id": "51r5wCAszKqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is technique similar to stemming. It reduces to lemma which is root-word rather than root stem. Best tech. Widely used. (chatbots etc use it)"
      ],
      "metadata": {
        "id": "SFxzBFEG0uTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "for word in wordlist:\n",
        "  print(word + \" ---->> \" + lemma.lemmatize(word, pos='n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPgigByDzGeK",
        "outputId": "3196262a-4a7c-4080-bc7c-eb4e4e594579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---->> eating\n",
            "eaten ---->> eaten\n",
            "eats ---->> eats\n",
            "writing ---->> writing\n",
            "write ---->> write\n",
            "program ---->> program\n",
            "programming ---->> programming\n",
            "history ---->> history\n",
            "finally ---->> finally\n",
            "finalized ---->> finalized\n",
            "do ---->> do\n",
            "don't ---->> don't\n",
            "fairly ---->> fairly\n",
            "sportingly ---->> sportingly\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ":param pos: The Part Of Speech tag.\n",
        "\n",
        "Valid options are \"n\" for nouns,\n",
        "    \"v\" for verbs, \"a\" for adjectives, \"r\" for adverbs and \"s\"\n",
        "for satellite adjectives."
      ],
      "metadata": {
        "id": "A_IVMK3S47N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in wordlist:\n",
        "  print(word + \" ---->> \" + lemma.lemmatize(word, pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umQmZXtUzXjL",
        "outputId": "736b9003-c97c-48b2-f05b-18dd16d7701a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---->> eat\n",
            "eaten ---->> eat\n",
            "eats ---->> eat\n",
            "writing ---->> write\n",
            "write ---->> write\n",
            "program ---->> program\n",
            "programming ---->> program\n",
            "history ---->> history\n",
            "finally ---->> finally\n",
            "finalized ---->> finalize\n",
            "do ---->> do\n",
            "don't ---->> don't\n",
            "fairly ---->> fairly\n",
            "sportingly ---->> sportingly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "takes time for very big sentences or paragraphs"
      ],
      "metadata": {
        "id": "BfHQPPKE5Hz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " ### Stop-words: for different use cases, the para we feed to the model is not completely useful.\n",
        "      Only some particular words play an important role in framing a response. Stop words are the words which\n",
        "      are not useful in the para and if removed wont affect the output.\n",
        "      there is already a list of stop words already present in nltk lib, but it is always advised to make ur own list\n",
        " '''\n",
        "\n",
        "para = \"\"\" Above all, My Lord, we want equal political rights, because without them our disabilities will be\n",
        "permanent. I know this sounds revolutionary to the whites in this country, because the majority of voters will\n",
        "be Africans. This makes the white man fear democracy.\n",
        "But this fear cannot be allowed to stand in the way of the only solution which will guarantee racial harmony\n",
        "and freedom for all. It is not true that the enfranchisement of all will result in racial domination.\n",
        "Political division, based on colour, is entirely artificial and, when it disappears, so will the domination\n",
        "of one colour group by another. The ANC has spent half a century fighting against racialism. When it triumphs\n",
        "as it certainly must, it will not change that policy.\n",
        "This then is what the ANC is fighting. Our struggle is a truly national one. It is a struggle of the African\n",
        "people, inspired by our own suffering and our own experience. It is a struggle for the right to live.\n",
        "During my lifetime I have dedicated my life to this struggle of the African people. I have fought against\n",
        "white domination, and I have fought against black domination. I have cherished the ideal of a democratic\n",
        "and free society in which all persons will live together in harmony and with equal opportunities. It is an\n",
        "ideal for which I hope to live for and to see realised. But, My Lord, if it needs be, it is an ideal for\n",
        "which I am prepared to die. \"\"\"\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "iL95T1yF4148"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english')) # list of enlgish stop words in nltk lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h35_sq37Zn1",
        "outputId": "8eeec4da-f5ef-4edd-e1cb-dd89f65d04c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snow_stem = SnowballStemmer('english')\n",
        "\n",
        "token_sent = sent_tokenize(para)\n",
        "token_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mTtnxNw8JZu",
        "outputId": "00150370-cb53-44fe-e873-321be00e9846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Above all, My Lord, we want equal political rights, because without them our disabilities will be \\npermanent.',\n",
              " 'I know this sounds revolutionary to the whites in this country, because the majority of voters will \\nbe Africans.',\n",
              " 'This makes the white man fear democracy.',\n",
              " 'But this fear cannot be allowed to stand in the way of the only solution which will guarantee racial harmony \\nand freedom for all.',\n",
              " 'It is not true that the enfranchisement of all will result in racial domination.',\n",
              " 'Political division, based on colour, is entirely artificial and, when it disappears, so will the domination \\nof one colour group by another.',\n",
              " 'The ANC has spent half a century fighting against racialism.',\n",
              " 'When it triumphs \\nas it certainly must, it will not change that policy.',\n",
              " 'This then is what the ANC is fighting.',\n",
              " 'Our struggle is a truly national one.',\n",
              " 'It is a struggle of the African \\npeople, inspired by our own suffering and our own experience.',\n",
              " 'It is a struggle for the right to live.',\n",
              " 'During my lifetime I have dedicated my life to this struggle of the African people.',\n",
              " 'I have fought against \\nwhite domination, and I have fought against black domination.',\n",
              " 'I have cherished the ideal of a democratic \\nand free society in which all persons will live together in harmony and with equal opportunities.',\n",
              " 'It is an \\nideal for which I hope to live for and to see realised.',\n",
              " 'But, My Lord, if it needs be, it is an ideal for \\nwhich I am prepared to die.']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stp_wrds = stopwords.words('english')\n",
        "set(stp_wrds)"
      ],
      "metadata": {
        "id": "FYIn9MKH-rSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply stop-words to filter and then do stemming\n",
        "import nltk\n",
        "\n",
        "for i in range(len(token_sent)):\n",
        "  words = nltk.word_tokenize(token_sent[i])\n",
        "\n",
        "  filtered_words = [snow_stem.stem(word) for word in words if word not in stp_wrds]\n",
        "  token_sent[i] = ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "GssPnzFn845X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_sentw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExbRmfrn_jc9",
        "outputId": "7e0ad39e-c40e-463a-f476-6a56c2c488e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abov , my lord , want equal polit right , without disabl perman .',\n",
              " 'i know sound revolutionari white countri , major voter african .',\n",
              " 'this make white man fear democraci .',\n",
              " 'but fear allow stand way solut guarante racial harmoni freedom .',\n",
              " 'it true enfranchis result racial domin .',\n",
              " 'polit divis , base colour , entir artifici , disappear , domin one colour group anoth .',\n",
              " 'the anc spent half centuri fight racial .',\n",
              " 'when triumph certain must , chang polici .',\n",
              " 'this anc fight .',\n",
              " 'our struggl truli nation one .',\n",
              " 'it struggl african peopl , inspir suffer experi .',\n",
              " 'it struggl right live .',\n",
              " 'dure lifetim i dedic life struggl african peopl .',\n",
              " 'i fought white domin , i fought black domin .',\n",
              " 'i cherish ideal democrat free societi person live togeth harmoni equal opportun .',\n",
              " 'it ideal i hope live see realis .',\n",
              " 'but , my lord , need , ideal i prepar die .']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "tZFb3dn8Dy6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(token_sent)):\n",
        "  words = nltk.word_tokenize(token_sent[i])\n",
        "\n",
        "  filtered_words = [lemma.lemmatize(word, pos='v') for word in words if word not in stp_wrds]\n",
        "  token_sent[i] = ' '.join(filtered_words)\n",
        "\n",
        "token_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMh7KM_dBiDf",
        "outputId": "49776b74-53ea-4439-f2f7-46e30c80408f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Above , My Lord , want equal political right , without disability permanent .',\n",
              " 'I know sound revolutionary white country , majority voter Africans .',\n",
              " 'This make white man fear democracy .',\n",
              " 'But fear allow stand way solution guarantee racial harmony freedom .',\n",
              " 'It true enfranchisement result racial domination .',\n",
              " 'Political division , base colour , entirely artificial , disappear , domination one colour group another .',\n",
              " 'The ANC spend half century fight racialism .',\n",
              " 'When triumph certainly must , change policy .',\n",
              " 'This ANC fight .',\n",
              " 'Our struggle truly national one .',\n",
              " 'It struggle African people , inspire suffer experience .',\n",
              " 'It struggle right live .',\n",
              " 'During lifetime I dedicate life struggle African people .',\n",
              " 'I fight white domination , I fight black domination .',\n",
              " 'I cherish ideal democratic free society person live together harmony equal opportunity .',\n",
              " 'It ideal I hope live see realise .',\n",
              " 'But , My Lord , need , ideal I prepare die .']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ### POS tag i.e. parts of speech tag help to identify all the differnet named entitties in a sentence\n",
        "# no stemming or stop-word removal is needed since we only need to identify\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sent = nltk.sent_tokenize(para)\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnebQab6DsHY",
        "outputId": "22650bb7-7e82-4426-fee3-568432eeeec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Above all, My Lord, we want equal political rights, because without them our disabilities will be \\npermanent.',\n",
              " 'I know this sounds revolutionary to the whites in this country, because the majority of voters will \\nbe Africans.',\n",
              " 'This makes the white man fear democracy.',\n",
              " 'But this fear cannot be allowed to stand in the way of the only solution which will guarantee racial harmony \\nand freedom for all.',\n",
              " 'It is not true that the enfranchisement of all will result in racial domination.',\n",
              " 'Political division, based on colour, is entirely artificial and, when it disappears, so will the domination \\nof one colour group by another.',\n",
              " 'The ANC has spent half a century fighting against racialism.',\n",
              " 'When it triumphs \\nas it certainly must, it will not change that policy.',\n",
              " 'This then is what the ANC is fighting.',\n",
              " 'Our struggle is a truly national one.',\n",
              " 'It is a struggle of the African \\npeople, inspired by our own suffering and our own experience.',\n",
              " 'It is a struggle for the right to live.',\n",
              " 'During my lifetime I have dedicated my life to this struggle of the African people.',\n",
              " 'I have fought against \\nwhite domination, and I have fought against black domination.',\n",
              " 'I have cherished the ideal of a democratic \\nand free society in which all persons will live together in harmony and with equal opportunities.',\n",
              " 'It is an \\nideal for which I hope to live for and to see realised.',\n",
              " 'But, My Lord, if it needs be, it is an ideal for \\nwhich I am prepared to die.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOtQ8TkkJ0DK",
        "outputId": "fa85d19b-a60f-4ea5-8cec-034eea03cc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sent)):\n",
        "  words = nltk.word_tokenize(sent[i])\n",
        "  pos_tag = nltk.pos_tag(words)\n",
        "  print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D3Y6pbDI85k",
        "outputId": "ea47cc70-bb73-403d-cadc-e6530f719214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Above', 'IN'), ('all', 'DT'), (',', ','), ('My', 'NNP'), ('Lord', 'NNP'), (',', ','), ('we', 'PRP'), ('want', 'VBP'), ('equal', 'JJ'), ('political', 'JJ'), ('rights', 'NNS'), (',', ','), ('because', 'IN'), ('without', 'IN'), ('them', 'PRP'), ('our', 'PRP$'), ('disabilities', 'NNS'), ('will', 'MD'), ('be', 'VB'), ('permanent', 'JJ'), ('.', '.')]\n",
            "[('I', 'PRP'), ('know', 'VBP'), ('this', 'DT'), ('sounds', 'VBZ'), ('revolutionary', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('whites', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('country', 'NN'), (',', ','), ('because', 'IN'), ('the', 'DT'), ('majority', 'NN'), ('of', 'IN'), ('voters', 'NNS'), ('will', 'MD'), ('be', 'VB'), ('Africans', 'NNPS'), ('.', '.')]\n",
            "[('This', 'DT'), ('makes', 'VBZ'), ('the', 'DT'), ('white', 'JJ'), ('man', 'NN'), ('fear', 'NN'), ('democracy', 'NN'), ('.', '.')]\n",
            "[('But', 'CC'), ('this', 'DT'), ('fear', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('allowed', 'VBN'), ('to', 'TO'), ('stand', 'VB'), ('in', 'IN'), ('the', 'DT'), ('way', 'NN'), ('of', 'IN'), ('the', 'DT'), ('only', 'JJ'), ('solution', 'NN'), ('which', 'WDT'), ('will', 'MD'), ('guarantee', 'VB'), ('racial', 'JJ'), ('harmony', 'NN'), ('and', 'CC'), ('freedom', 'NN'), ('for', 'IN'), ('all', 'DT'), ('.', '.')]\n",
            "[('It', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('true', 'JJ'), ('that', 'IN'), ('the', 'DT'), ('enfranchisement', 'NN'), ('of', 'IN'), ('all', 'DT'), ('will', 'MD'), ('result', 'VB'), ('in', 'IN'), ('racial', 'JJ'), ('domination', 'NN'), ('.', '.')]\n",
            "[('Political', 'JJ'), ('division', 'NN'), (',', ','), ('based', 'VBN'), ('on', 'IN'), ('colour', 'NN'), (',', ','), ('is', 'VBZ'), ('entirely', 'RB'), ('artificial', 'JJ'), ('and', 'CC'), (',', ','), ('when', 'WRB'), ('it', 'PRP'), ('disappears', 'VBZ'), (',', ','), ('so', 'RB'), ('will', 'MD'), ('the', 'DT'), ('domination', 'NN'), ('of', 'IN'), ('one', 'CD'), ('colour', 'NN'), ('group', 'NN'), ('by', 'IN'), ('another', 'DT'), ('.', '.')]\n",
            "[('The', 'DT'), ('ANC', 'NNP'), ('has', 'VBZ'), ('spent', 'VBN'), ('half', 'PDT'), ('a', 'DT'), ('century', 'NN'), ('fighting', 'VBG'), ('against', 'IN'), ('racialism', 'NN'), ('.', '.')]\n",
            "[('When', 'WRB'), ('it', 'PRP'), ('triumphs', 'VBZ'), ('as', 'IN'), ('it', 'PRP'), ('certainly', 'RB'), ('must', 'MD'), (',', ','), ('it', 'PRP'), ('will', 'MD'), ('not', 'RB'), ('change', 'VB'), ('that', 'DT'), ('policy', 'NN'), ('.', '.')]\n",
            "[('This', 'DT'), ('then', 'RB'), ('is', 'VBZ'), ('what', 'WP'), ('the', 'DT'), ('ANC', 'NNP'), ('is', 'VBZ'), ('fighting', 'VBG'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('struggle', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('truly', 'JJ'), ('national', 'JJ'), ('one', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('struggle', 'NN'), ('of', 'IN'), ('the', 'DT'), ('African', 'JJ'), ('people', 'NNS'), (',', ','), ('inspired', 'VBN'), ('by', 'IN'), ('our', 'PRP$'), ('own', 'JJ'), ('suffering', 'NN'), ('and', 'CC'), ('our', 'PRP$'), ('own', 'JJ'), ('experience', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('struggle', 'NN'), ('for', 'IN'), ('the', 'DT'), ('right', 'NN'), ('to', 'TO'), ('live', 'VB'), ('.', '.')]\n",
            "[('During', 'IN'), ('my', 'PRP$'), ('lifetime', 'NN'), ('I', 'PRP'), ('have', 'VBP'), ('dedicated', 'VBN'), ('my', 'PRP$'), ('life', 'NN'), ('to', 'TO'), ('this', 'DT'), ('struggle', 'NN'), ('of', 'IN'), ('the', 'DT'), ('African', 'JJ'), ('people', 'NNS'), ('.', '.')]\n",
            "[('I', 'PRP'), ('have', 'VBP'), ('fought', 'VBN'), ('against', 'IN'), ('white', 'JJ'), ('domination', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('have', 'VBP'), ('fought', 'VBN'), ('against', 'IN'), ('black', 'JJ'), ('domination', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('have', 'VBP'), ('cherished', 'VBN'), ('the', 'DT'), ('ideal', 'NN'), ('of', 'IN'), ('a', 'DT'), ('democratic', 'JJ'), ('and', 'CC'), ('free', 'JJ'), ('society', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('all', 'DT'), ('persons', 'NNS'), ('will', 'MD'), ('live', 'VB'), ('together', 'RB'), ('in', 'IN'), ('harmony', 'NN'), ('and', 'CC'), ('with', 'IN'), ('equal', 'JJ'), ('opportunities', 'NNS'), ('.', '.')]\n",
            "[('It', 'PRP'), ('is', 'VBZ'), ('an', 'DT'), ('ideal', 'NN'), ('for', 'IN'), ('which', 'WDT'), ('I', 'PRP'), ('hope', 'VBP'), ('to', 'TO'), ('live', 'VB'), ('for', 'IN'), ('and', 'CC'), ('to', 'TO'), ('see', 'VB'), ('realised', 'JJ'), ('.', '.')]\n",
            "[('But', 'CC'), (',', ','), ('My', 'NNP'), ('Lord', 'NNP'), (',', ','), ('if', 'IN'), ('it', 'PRP'), ('needs', 'VBZ'), ('be', 'VB'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('an', 'DT'), ('ideal', 'NN'), ('for', 'IN'), ('which', 'WDT'), ('I', 'PRP'), ('am', 'VBP'), ('prepared', 'JJ'), ('to', 'TO'), ('die', 'VB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"My name is Tanmay Ekade. I am in 3rd year at IIT Kharagpur. I am from Nashik, MH. It is India's wine capital!\""
      ],
      "metadata": {
        "id": "88loHumcJxjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48BJpRyIK2yc",
        "outputId": "a754bb51-711c-4b35-87ba-f9d4993b9628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'name', 'is', 'Tanmay', 'Ekade.', 'I', 'am', 'in', '3rd', 'year', 'at', 'IIT', 'Kharagpur.', 'I', 'am', 'from', 'Nashik,', 'MH.', 'It', 'is', \"India's\", 'wine', 'capital!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = sent.split()"
      ],
      "metadata": {
        "id": "0GGfcCTpLEPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.pos_tag(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFhkqWNjK4Mh",
        "outputId": "e9622116-c708-437e-d4a4-c789b51ee2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Tanmay', 'NNP'), ('Ekade.', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('in', 'IN'), ('3rd', 'CD'), ('year', 'NN'), ('at', 'IN'), ('IIT', 'NNP'), ('Kharagpur.', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Nashik,', 'NNP'), ('MH.', 'NNP'), ('It', 'PRP'), ('is', 'VBZ'), (\"India's\", 'NNP'), ('wine', 'NN'), ('capital!', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Named entity recognition --> In a sentence different parts are highlighted based on it type\n",
        "# name: tanmay || place or location: nashik || date: 29/11/2003 || time: 3.24pm || money, percent, etc.\n",
        "\n",
        "sent = \"Hello! Tanmay here. I am a 3rd year undergraudate at IIT Kharagpur. I am from Nashik, Maharashtra\"\n",
        "\n",
        "import nltk\n",
        "words = nltk.word_tokenize(sent)"
      ],
      "metadata": {
        "id": "gtPyN2a7LJSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_elemnts = nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "vKG54V8Y7pQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "id": "ugKr3Q428NnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "y9ysizlq8RFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to do named entity just run one line\n",
        "tree = nltk.ne_chunk(pos_elemnts)"
      ],
      "metadata": {
        "id": "n4WPqXXr7p9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "id": "25facsA7AZfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "display(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "nJZfyHxpANJX",
        "outputId": "ebb82541-190f-4414-f763-c416353537c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('GPE', [('Hello', 'NN')]), ('!', '.'), Tree('PERSON', [('Tanmay', 'NNP')]), ('here', 'RB'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('3rd', 'CD'), ('year', 'NN'), ('undergraudate', 'NN'), ('at', 'IN'), Tree('ORGANIZATION', [('IIT', 'NNP'), ('Kharagpur', 'NNP')]), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), Tree('GPE', [('Nashik', 'NNP')]), (',', ','), Tree('GPE', [('Maharashtra', 'NNP')])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1040.0,168.0\" width=\"1040px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"5.38462%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Hello</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.69231%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.30769%\" x=\"5.38462%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">!</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.53846%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.15385%\" x=\"7.69231%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tanmay</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.7692%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.61538%\" x=\"13.8462%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">here</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.1538%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.30769%\" x=\"18.4615%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.6154%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.84615%\" x=\"20.7692%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.6923%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.84615%\" x=\"24.6154%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">am</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.5385%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.07692%\" x=\"28.4615%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.84615%\" x=\"31.5385%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">3rd</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.4615%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.61538%\" x=\"35.3846%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">year</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.6923%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"11.5385%\" x=\"40%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">undergraudate</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.7692%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.07692%\" x=\"51.5385%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0769%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"12.3077%\" x=\"54.6154%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"31.25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IIT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"68.75%\" x=\"31.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Kharagpur</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.625%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.7692%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.30769%\" x=\"66.9231%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.0769%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.84615%\" x=\"69.2308%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.1538%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.84615%\" x=\"73.0769%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">am</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.61538%\" x=\"76.9231%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.2308%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.15385%\" x=\"81.5385%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Nashik</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.6154%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.30769%\" x=\"87.6923%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.8462%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10%\" x=\"90%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Maharashtra</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the above methods described are the \"Text Pre-processing\" ones. Now we proceed to convert this text to vectors and then finally feed them to our model.\n",
        "\n",
        "\n",
        "General Methods for Text --> Vector:\n",
        "1.   One hot encoding (dont use generally): sklearn= onehotencoder and pandas= pd.get_dummies\n",
        "2.   bag of words (bow) used frequently\n",
        "3.   TF-IDF (term freq - inverse document freq.)\n",
        "4.    Word2Vec\n",
        "5.   AvgWord2Vec"
      ],
      "metadata": {
        "id": "_YvYAUJmA0dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### bag of words BOW\n",
        "'''\n",
        " suppose we have some trai ning data as few sentences:\n",
        " 1) He is a good boy || 2) She is a good girl || 3) Boy and girl are good || etc.\n",
        "\n",
        "we first proceed by lowering all sentences (str.lower()) and then applying stop-words to get only necessary ones\n",
        "then on the remaining words we define a vocabulary which is basically the set of all unique words. remaining ex:\n",
        "1) good boy || 2) good girl || 3) boy girl good. (notice everything is lowercase)\n",
        "\n",
        "now we proceed to create a vocabulary, frequency = {[good, 3], [boy, 2], [girl, 2]}\n",
        "arrange this vocab, freq in descending order and now convert each of the sentences to vectors like:\n",
        "   good, boy, girl.  # 2 types of BOW based on freq: binary and normal. if in sent a specific word occurs more than 1 time then in binary count wont matter always 1. but in normal BOW we set it equal to count. (ex: good 2)\n",
        "1) [1, 1, 0]\n",
        "2) [1, 0, 1]\n",
        "3) [1, 1, 1]\n",
        "\n",
        "we get the required vectors for each of the sentences\n",
        "'''\n",
        "\n",
        "# disadvantages\n",
        "'''\n",
        "1. sparse matrix/array (causes overfitting) 2. ordering of words changed 3. words out of vocabulary not considered 3. semantic meaning is not captured\n",
        "\n",
        "for 3. --> consider two sent: 1) The food is good and 2) The food is not good\n",
        "there are opposites of each other i.e. not similar in any way. If using BOW we try to get vectors they are as follows\n",
        "1) [1, 1, 1, 0, 1]   0 is for not\n",
        "2) [1, 1, 1, 1, 1]\n",
        "\n",
        "so if we find the cosine dist/similarity we wont find a large difference since only 1 number is changing but\n",
        "in reality the sent are opp to each other. hence disadvantage\n",
        "'''"
      ],
      "metadata": {
        "id": "MbNluKiSArPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TF-IDF\n",
        "'''\n",
        "Sentences: 1) good boy || 2) good girl || 3) boy girl good\n",
        "\n",
        "Term freq(TF) = no of times word repeating in sent / no of words in sent\n",
        "\n",
        "IDF(inverse document freq.) = ln(no of sent / no of sent containing the word)\n",
        "\n",
        "vocab: [good boy girl]  TF for each word in sent is-->\n",
        "s1: 1/2, 1/2, 0\n",
        "s2: 1/2, 0, 1/2\n",
        "s3: 1/3, 1/3, 1/3\n",
        "\n",
        "IDF for each word -->\n",
        "good: ln(3/3)=0\n",
        "boy: ln(3/2)\n",
        "girl: ln(3/2)\n",
        "\n",
        "Final vector for each sent is TF*IDF element-wise or word-wise:\n",
        "Ex- TF s1=[1/2(good), 1/2(boy), 0(girl)] * IDF [0, ln(3/2), ln(3/2)]\n",
        "hence final vector using above is -->\n",
        "\n",
        "s1 = [0, 1/2*ln(3/2), 0]\n",
        "s2 = [0, 0, 1/2*ln(3/2)]\n",
        "s3 = [0, 1/2*ln(3/2), 1/2*ln(3/2)]\n",
        "'''\n",
        "\n",
        "### adv and disadvantages\n",
        "'''\n",
        "as we can see the word importance is getting captured effectively. most common is least imp (good here)\n",
        "\n",
        "disadv is sparsity still exists and other one that words out of vocab still not counted\n",
        "'''"
      ],
      "metadata": {
        "id": "0AVWsTpUKOVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### word2vec --> deep learning pre-trained model\n",
        "'''\n",
        "a deep learning model is already trained on a very large vocabulary of words (billions of words)\n",
        "when we pass our unique vocab to this model, then it assigns weights to each of our words based on its association\n",
        "with the already pre-trained ones.\n",
        "\n",
        "word2vec it of two types: 1) CBOW (continuous bag of words)  2) Skipgram\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Vfljr3s5Bnsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZHqiOt3pbfco"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}